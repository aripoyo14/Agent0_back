import json
from typing import Dict, Any, List, Union
from sqlalchemy.orm import Session
from app.services.vector import index, embeddings
from datetime import datetime, timezone, timedelta

# æ—¥æœ¬æ¨™æº–æ™‚ï¼ˆJSTï¼‰
JST = timezone(timedelta(hours=9))

class SummaryVectorService:
    """è¦ç´„å†…å®¹ã®ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã¨Pineconeä¿å­˜ã‚’ç®¡ç†ã™ã‚‹ã‚µãƒ¼ãƒ“ã‚¹"""

    def __init__(self):
        self.namespace = "summaries"
        self.vector_dimension = 1536  # OpenAI text-embedding-ada-002ã®æ¬¡å…ƒæ•°

    def vectorize_summary(
        self, 
        summary_title: str, 
        summary_content: str, 
        expert_id: int, 
        tag_ids: Union[int, List[int], str],
        summary_id: str = None
    ) -> Dict[str, Any]:
        """
        è¦ç´„å†…å®¹ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ã¦Pineconeã«ä¿å­˜ã™ã‚‹
        
        Args:
            summary_title: è¦ç´„ã®ã‚¿ã‚¤ãƒˆãƒ«
            summary_content: è¦ç´„ã®å†…å®¹
            expert_id: ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆID
            tag_ids: ã‚¿ã‚°IDï¼ˆå˜ä¸€ã®intã€ãƒªã‚¹ãƒˆã€ã¾ãŸã¯ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®æ–‡å­—åˆ—ï¼‰
            summary_id: è¦ç´„IDï¼ˆæŒ‡å®šã•ã‚Œãªã„å ´åˆã¯è‡ªå‹•ç”Ÿæˆï¼‰
            
        Returns:
            Dict[str, Any]: å‡¦ç†çµæœã®è©³ç´°
        """
        try:
            # summary_idãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯è‡ªå‹•ç”Ÿæˆ
            if summary_id is None:
                summary_id = f"summary_{datetime.now(JST).strftime('%Y%m%d_%H%M%S')}"
            
            # tag_idsã‚’æ­£è¦åŒ–ï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®æ–‡å­—åˆ—ã«å¤‰æ›ï¼‰
            tag_ids_str = self._normalize_tag_ids(tag_ids)
            
            # ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½œæˆ
            text_to_embed = f"Title: {summary_title}, Summary: {summary_content}, Expert ID: {expert_id}, Tag IDs: {tag_ids_str}"
            
            # ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–
            print(f"ğŸ” è¦ç´„å†…å®¹ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ä¸­...")
            embedding = embeddings.embed_query(text_to_embed)
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
            metadata = {
                "summary_id": summary_id,
                "title": summary_title,
                "summary": summary_content,
                "expert_id": expert_id,
                "tag_ids": tag_ids_str,  # ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®æ–‡å­—åˆ—
                "type": "summary",
                "created_at": datetime.now(JST).isoformat()
            }
            
            # Pineconeã«ãƒ™ã‚¯ãƒˆãƒ«ã‚’ä¿å­˜
            vector_data = {
                "id": summary_id,
                "values": embedding,
                "metadata": metadata
            }
            
            print(f"ğŸ’¾ Pineconeã«è¦ç´„ãƒ™ã‚¯ãƒˆãƒ«ã‚’ä¿å­˜ä¸­...")
            index.upsert(
                vectors=[vector_data],
                namespace=self.namespace
            )

            return {
                "success": True,
                "message": f"è¦ç´„å†…å®¹ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ã¦Pineconeã«ä¿å­˜ã—ã¾ã—ãŸ",
                "summary_id": summary_id,
                "namespace": self.namespace,
                "metadata": metadata
            }

        except Exception as e:
            print(f"âŒ è¦ç´„ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return {
                "success": False,
                "message": f"ãƒ™ã‚¯ãƒˆãƒ«åŒ–å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
            }

    def _normalize_tag_ids(self, tag_ids: Union[int, List[int], str]) -> str:
        """
        tag_idsã‚’æ­£è¦åŒ–ã—ã¦ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®æ–‡å­—åˆ—ã«å¤‰æ›ã™ã‚‹
        
        Args:
            tag_ids: ã‚¿ã‚°IDï¼ˆå˜ä¸€ã®intã€ãƒªã‚¹ãƒˆã€ã¾ãŸã¯ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®æ–‡å­—åˆ—ï¼‰
            
        Returns:
            str: ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®ã‚¿ã‚°IDæ–‡å­—åˆ—
        """
        if isinstance(tag_ids, int):
            return str(tag_ids)
        elif isinstance(tag_ids, list):
            return ",".join(map(str, tag_ids))
        elif isinstance(tag_ids, str):
            # æ—¢ã«ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®æ–‡å­—åˆ—ã®å ´åˆã¯ãã®ã¾ã¾è¿”ã™
            return tag_ids
        else:
            raise ValueError(f"Unsupported tag_ids type: {type(tag_ids)}")

    def _parse_tag_ids(self, tag_ids_str: str) -> List[int]:
        """
        ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®ã‚¿ã‚°IDæ–‡å­—åˆ—ã‚’æ•´æ•°ãƒªã‚¹ãƒˆã«å¤‰æ›ã™ã‚‹
        
        Args:
            tag_ids_str: ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®ã‚¿ã‚°IDæ–‡å­—åˆ—
            
        Returns:
            List[int]: ã‚¿ã‚°IDã®æ•´æ•°ãƒªã‚¹ãƒˆ
        """
        if not tag_ids_str:
            return []
        return [int(tag_id.strip()) for tag_id in tag_ids_str.split(",") if tag_id.strip()]

    def search_similar_summaries(
        self, 
        query: str, 
        top_k: int = 5,
        expert_id: int = None,
        tag_ids: Union[int, List[int], str] = None
    ) -> List[Dict[str, Any]]:
        """
        ã‚¯ã‚¨ãƒªã«é¡ä¼¼ã—ãŸè¦ç´„ã‚’æ¤œç´¢ã™ã‚‹
        
        Args:
            query (str): æ¤œç´¢ã‚¯ã‚¨ãƒª
            top_k (int): è¿”ã™çµæœã®æ•°
            expert_id (int, optional): ç‰¹å®šã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã§çµã‚Šè¾¼ã¿
            tag_ids (Union[int, List[int], str], optional): ç‰¹å®šã®ã‚¿ã‚°ã§çµã‚Šè¾¼ã¿
            
        Returns:
            List[Dict[str, Any]]: é¡ä¼¼ã—ãŸè¦ç´„ã®ãƒªã‚¹ãƒˆ
        """
        try:
            # ã‚¯ã‚¨ãƒªã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–
            query_embedding = embeddings.embed_query(query)
            
            # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶ã‚’æº–å‚™
            filter_dict = {}
            if expert_id is not None:
                filter_dict["expert_id"] = expert_id
            
            # Pineconeã§é¡ä¼¼æ¤œç´¢
            results = index.query(
                vector=query_embedding,
                namespace=self.namespace,
                top_k=top_k,
                include_metadata=True,
                filter=filter_dict if filter_dict else None
            )
            
            # çµæœã‚’æ•´å½¢
            similar_summaries = []
            for match in results.matches:
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚¿ã‚°IDã‚’å–å¾—
                tag_ids_str = match.metadata.get("tag_ids", "")
                tag_ids_list = self._parse_tag_ids(tag_ids_str)
                
                # ã‚¿ã‚°IDãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯çµã‚Šè¾¼ã¿
                if tag_ids is not None:
                    search_tag_ids = self._parse_tag_ids(self._normalize_tag_ids(tag_ids))
                    # å…±é€šã®ã‚¿ã‚°IDãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                    if not any(tag_id in tag_ids_list for tag_id in search_tag_ids):
                        continue
                
                similar_summaries.append({
                    "summary_id": match.metadata.get("summary_id"),
                    "title": match.metadata.get("title"),
                    "summary": match.metadata.get("summary"),
                    "expert_id": match.metadata.get("expert_id"),
                    "tag_ids": tag_ids_list,  # æ•´æ•°ãƒªã‚¹ãƒˆã¨ã—ã¦è¿”ã™
                    "tag_ids_str": tag_ids_str,  # å…ƒã®æ–‡å­—åˆ—ã‚‚ä¿æŒ
                    "score": match.score,
                    "created_at": match.metadata.get("created_at")
                })
            
            return similar_summaries

        except Exception as e:
            print(f"âŒ è¦ç´„æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return []

    def delete_summary_vector(self, summary_id: str) -> Dict[str, Any]:
        """
        æŒ‡å®šã•ã‚ŒãŸè¦ç´„IDã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’å‰Šé™¤ã™ã‚‹
        
        Args:
            summary_id: å‰Šé™¤ã™ã‚‹è¦ç´„ã®ID
            
        Returns:
            Dict[str, Any]: å‡¦ç†çµæœã®è©³ç´°
        """
        try:
            # Pineconeã‹ã‚‰ãƒ™ã‚¯ãƒˆãƒ«ã‚’å‰Šé™¤
            index.delete(
                ids=[summary_id],
                namespace=self.namespace
            )
            
            return {
                "success": True,
                "message": f"è¦ç´„ãƒ™ã‚¯ãƒˆãƒ« (ID: {summary_id}) ã‚’å‰Šé™¤ã—ã¾ã—ãŸ",
                "summary_id": summary_id
            }

        except Exception as e:
            print(f"âŒ è¦ç´„ãƒ™ã‚¯ãƒˆãƒ«å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return {
                "success": False,
                "message": f"ãƒ™ã‚¯ãƒˆãƒ«å‰Šé™¤ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
            }

    def get_summary_vector_statistics(self) -> Dict[str, Any]:
        """
        Pineconeã®è¦ç´„ãƒ™ã‚¯ãƒˆãƒ«çµ±è¨ˆæƒ…å ±ã‚’å–å¾—ã™ã‚‹
        
        Returns:
            Dict[str, Any]: çµ±è¨ˆæƒ…å ±
        """
        try:
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®çµ±è¨ˆæƒ…å ±ã‚’å–å¾—
            stats = index.describe_index_stats()
            
            # summariesãƒãƒ¼ãƒ ã‚¹ãƒšãƒ¼ã‚¹ã®æƒ…å ±ã‚’å–å¾—
            namespace_stats = stats.namespaces.get(self.namespace, {})
            
            return {
                "success": True,
                "total_vector_count": stats.total_vector_count,
                "summaries_vector_count": namespace_stats.get("vector_count", 0),
                "dimension": stats.dimension,
                "index_fullness": stats.index_fullness,
                "namespaces": list(stats.namespaces.keys())
            }

        except Exception as e:
            print(f"âŒ çµ±è¨ˆæƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return {
                "success": False,
                "message": f"çµ±è¨ˆæƒ…å ±å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
            }

# ã‚µãƒ¼ãƒ“ã‚¹ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
summary_vector_service = SummaryVectorService()
